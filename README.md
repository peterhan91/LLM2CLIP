# LLM2CLIP: Extending the Capability Boundaries of CLIP through Large Language Models
## Introduction
This is the official implementation of LLM2CLIP. LLM2CLIP aims to embrace the power of LLMs to unlock CLIPâ€™s potential. By fine-tuning the LLM in the caption space with contrastive learning, we extract its textual capabilities into the output embeddings, significantly improving the output layerâ€™s textual discriminability.

## News ğŸš€ğŸš€ğŸš€
## Model Zoo (Coming Soon) 
## ğŸ’» How to Install
```
conda create -n llm2clip python=3.8
conda activate llm2clip

pip install -r requirements.txt
```
### Data Preparation (Coming Soon) 
### ğŸ”¥ Training  
```sh run.sh```

## â¤ï¸ Acknowlegement

Our code is built on top of [eva-clip](https://github.com/baaivision/EVA/tree/master/EVA-CLIP). Thanks for their nice work!